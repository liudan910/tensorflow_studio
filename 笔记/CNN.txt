一 激活函数
加入非线性因素
二  CNN结构
输入层
卷积层：使用卷积核进行特征提取和特征映射
激励层：由于卷积也是一种线性运算，因此需要增加非线性因素
池化层：进行下采样，对特征图稀疏处理，减少数据运算量
全连接层：通常在CNN的尾部进行重新拟合，减少特征信息的损失
输出层

中间可加入的功能层：
归一化层：在CNN中对特征的归一化
切分层：对某些（图片）数据进行分区域的单独学习
融合层: 对独立 进行特征学习的分支进行融合。

例：一个分类模型完整的结构
卷积层：          使用卷积核进行特征提取和特征映射。 维度：一个例样为几维，卷积核就是几维。如一句话以一维向量表示，则conv1d,如一个图像二维向量表示，则conv2d；核数目=一个输入所需的特征数。 核大小=3，5，7，11（3开始的质数，图片以3居多，文本以3，5居多）
池化层:           下采样，对特征图稀疏处理，减少数据运算量
全连接层：        通常在CNN尾部进行重新拟合 ，减少特征信息的损失   。 神经元数目即该层的对于一个输入的输出数目，64，128，256； (2的n次方）
dropout层：       加入随机选择特性，避免过拟合。 只在训练过程中配置keep_pro=0.5,使得反射传播只会对部分数据起作用。而在测试评估时keep_pro=1，因为测试评估时需要使用全部数据。
relu层:           加入非线性因素。
全连接层          用于分类。         神经元数目对应于分类的类别数。

卷积层与池化层：  卷积层后面大多都会跟一个池化层。
全连接层与relu层： 因为全连接是线性运算， relu是加入非线性因素，故在一个全连接层后面跟一个relu即可以。
dropout与relu层先后顺序：  dropout层是用于随机选择，relu=max(0,x)，故dropout和relu可以不分先后。
卷积层与全连接层： 一般卷积层放在前面，进行特征提取和映射 。全连接层放后面进行重新拟合，减少特征信息的损失。

说到底卷积层与全连接层的作用不同之处在于：
  前者是特征提取和映射； （高级特征提取）
  后者是拟合 。



